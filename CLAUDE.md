# AI Audit Platform - Workflow-Driven Development Guide

> **Version**: 1.2.0
> **Last Updated**: 2026-01-06
> **Purpose**: Comprehensive workflow guide with concrete tool sequences for Claude Code development

---

## ğŸ“‹ Table of Contents

1. [How This Guide Works](#how-this-guide-works)
2. [Quick Start Guide](#quick-start-guide)
3. [Core Workflows](#core-workflows)
4. [Subagent Orchestration](#subagent-orchestration)
5. [Global Infrastructure](#global-infrastructure)
6. [MCP Integration](#mcp-integration)
7. [Project Context](#project-context)
8. [Feedback & Improvement](#feedback--improvement)
9. [Emergency Procedures](#emergency-procedures)
10. [Best Practices](#best-practices)

---

## How This Guide Works

### Session Lifecycle & Auto-Loading

**CRITICAL**: This CLAUDE.md file is **automatically loaded** at the start of EVERY session:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Session Start (New or Resumed)         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  CLAUDE.md Auto-Loaded                  â”‚
â”‚  (via system context injection)         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Session Initialization Hook            â”‚
â”‚  (reads metrics, git status, etc.)      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Ready to Execute Workflows             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**When CLAUDE.md is loaded:**
- âœ“ Every new session start
- âœ“ When conversation context is summarized (token limit reached)
- âœ“ When session is resumed after interruption
- âœ“ **NO explicit user request needed** - it's automatic

**What this means:**
- All workflows, slash commands, and best practices are ALWAYS available
- Claude automatically follows these guidelines without being told
- Consistency across all sessions guaranteed

---

### Automatic Tool & Technology Usage

**MCP Servers** (Model Context Protocol):

Claude automatically uses MCP servers **WITHOUT explicit user request** when appropriate:

| MCP Server | Auto-Use Trigger | Purpose |
|------------|------------------|---------|
| **Context7** | Documentation lookup needed | Up-to-date library docs (React, Next.js, etc.) |
| **Greptile** | PR/code review requested | GitHub PR analysis, code review |
| **Playwright** | Browser automation needed | Web scraping, UI testing |

**Example - Automatic MCP Usage**:
```
User: "How do I use React 19's new useActionState hook?"
Claude: [Automatically uses Context7 MCP to get latest React docs]
        [NO need for user to say "use Context7" or "/context7"]
```

**Slash Commands & Skills**:

Claude automatically invokes slash commands/skills when the task matches their purpose:

```
User: "Implement a dashboard feature"
Claude: [Automatically executes /plan-implement-verify workflow]
        [NO need for user to type "/plan-implement-verify"]

User: "Commit my changes"
Claude: [Automatically executes /commit-push-pr]
        [NO need for user to type "/commit-push-pr"]

User: "Review my code quality"
Claude: [Automatically executes /validate-architecture]
        [NO need for user to type "/validate-architecture"]
```

**Auto-Invoked Skills**:

Claude automatically uses skills from three sources:
1. **SuperClaude Skills** (`/sc:*`) - Advanced orchestration and analysis
2. **Installed Plugins** - Official and community plugins
3. **Custom User Skills** - Project-specific workflows

### SuperClaude Skills (Intelligent Orchestration)

| Skill | Auto-Use Trigger | Purpose |
|-------|------------------|---------|
| `/sc:implement` | Feature implementation request | Persona activation for implementation with MCP integration |
| `/sc:analyze` | Code analysis request | Comprehensive quality, security, performance, architecture analysis |
| `/sc:troubleshoot` | Bug/error diagnosis needed | Systematic diagnosis and resolution |
| `/sc:improve` | Code quality improvement | Apply systematic improvements (refactoring, optimization) |
| `/sc:explain` | Code explanation needed | Educational clarity on code/concepts/systems |
| `/sc:design` | Architecture design needed | Design system architecture, APIs, component interfaces |
| `/sc:test` | Testing request | Execute tests with coverage analysis and quality reporting |
| `/sc:research` | Research/investigation needed | Deep web research with adaptive planning |
| `/sc:workflow` | Complex multi-step task | Generate structured implementation workflows |
| `/sc:brainstorm` | Requirements discovery | Interactive Socratic dialogue for requirements |

### Installed Plugin Skills

| Skill | Auto-Use Trigger | Purpose |
|-------|------------------|---------|
| **Code Review & Development** | | |
| `code-review:code-review` | PR review request | Automated code review of pull requests |
| `feature-dev:feature-dev` | Complex feature development | Guided feature development with codebase understanding |
| `validate-architecture` | Before commit or quality check | Deep architecture analysis (OOP, clean code, file size) |
| **Document Generation** | | |
| `document-skills:pdf` | PDF manipulation needed | Extract, create, merge PDFs; fill forms |
| `document-skills:xlsx` | Spreadsheet work needed | Create/edit spreadsheets with formulas, formatting |
| `document-skills:pptx` | Presentation needed | Create/edit PowerPoint presentations |
| `document-skills:doc-coauthoring` | Documentation writing | Structured workflow for co-authoring docs, specs |
| `document-skills:theme-factory` | Styling artifacts | Apply themes to slides, docs, reports, HTML pages |
| **Notion Integration** | | |
| `Notion:notion-search` | Search Notion workspace | Find pages, databases in Notion |
| `Notion:notion-create-page` | Create Notion page | Add new pages to Notion workspace |
| `Notion:notion-create-task` | Create Notion task | Add tasks to Notion tasks database |
| `Notion:notion-database-query` | Query Notion database | Retrieve structured data from Notion databases |
| **Agent & Plugin Development** | | |
| `agent-sdk-dev:new-sdk-app` | Create Agent SDK app | Setup new Claude Agent SDK application (TypeScript/Python) |
| `plugin-dev:create-plugin` | Create plugin | End-to-end plugin creation with component design |
| **LLM & AI Application Development** | | |
| `llm-application-dev:ai-engineer` | LLM application/agent building | Production-ready LLM apps, RAG systems, intelligent agents |
| `llm-application-dev:prompt-engineer` | Prompt optimization needed | Advanced prompting, chain-of-thought, prompt strategies |
| `llm-application-dev:vector-database-engineer` | Vector search/embeddings | Vector databases, semantic search, embeddings optimization |

### Custom User Skills (Project Workflows)

| Skill | Auto-Use Trigger | Purpose |
|-------|------------------|---------|
| **Requirements Discovery** | | |
| `/interview` | **NEW feature OR architecture improvement** | **Deep requirements interview (RUNS FIRST, triggers plan mode)** |
| **Development Workflows** | | |
| `/worktree-setup` | Feature/bugfix start | Create isolated git worktree for parallel development |
| `/plan-implement-verify` | Feature implementation | Full 6-phase development cycle (plan â†’ implement â†’ validate) |
| `/validate-architecture` | Before commit | Architecture validation (unnecessary lines, OOP, file size) |
| `/commit-push-pr` | Ready to commit | Complete git workflow (stage â†’ commit â†’ push â†’ PR) |
| `/subagent-spawn` | Granular task delegation | Launch focused subagent for specific file/module task |
| `/feedback-capture` | After workflow completion | Record metrics for continuous improvement |

**CRITICAL: `/interview` Priority**

The `/interview` skill **MUST run FIRST** for:
- âœ“ New feature requests (any feature, regardless of complexity)
- âœ“ Architecture improvements or refactoring
- âœ“ Complex bug fixes with unclear requirements
- âœ“ When user requirements are ambiguous or incomplete

**What `/interview` Does**:
1. **Reads plan file** (if exists) to understand initial context
2. **Conducts deep interview** using AskUserQuestion tool:
   - Technical implementation details (API design, data structures, algorithms)
   - UI/UX design decisions (user flows, visual design, accessibility)
   - Potential concerns and edge cases (error handling, boundary conditions, race conditions)
   - Architecture and design tradeoffs (scalability, maintainability, performance)
3. **Asks non-superficial questions** - goes deep, not obvious/shallow
4. **Continues until complete understanding** - doesn't stop prematurely
5. **Writes comprehensive specification document** - detailed spec with all decisions documented
6. **Enters plan mode** automatically after interview completion

**Interview â†’ Plan â†’ Implement Flow**:
```
User: "Implement a real-time notification system"
  â†“
STEP 1: /interview (AUTOMATIC, FIRST)
  â†’ Ask: "What types of notifications? (push, email, in-app?)"
  â†’ Ask: "How should notifications be prioritized?"
  â†’ Ask: "What happens if user is offline?"
  â†’ Ask: "Should notifications be grouped/batched?"
  â†’ Ask: "What's the expected notification volume per user?"
  â†’ Ask: "How do we handle notification permissions?"
  â†’ ... (continues until complete understanding)
  â†’ Output: comprehensive-notification-spec.md
  â†“
STEP 2: Enter Plan Mode (AUTOMATIC)
  â†’ Use spec to design architecture
  â†’ Break down into 5-8 granular tasks
  â†’ Get user approval
  â†“
STEP 3: /plan-implement-verify
  â†’ Execute implementation with full context
```

**Why Interview First?**
- âŒ **Without interview**: Assumptions, missing requirements, rework
- âœ“ **With interview**: Clear requirements, informed decisions, correct implementation first time

**Example Questions from `/interview`**:

```
BAD (superficial):
âŒ "Should we add a dashboard?"
âŒ "Do you want this to be fast?"
âŒ "Should it look good?"

GOOD (deep, technical):
âœ“ "What metrics are most critical for users to see at-a-glance vs. drill-down?"
âœ“ "How should we handle data refresh - polling interval, WebSocket, SSE?"
âœ“ "What's the acceptable latency for real-time updates - <100ms, <1s, <5s?"
âœ“ "How do we gracefully degrade if the backend is slow/unavailable?"
âœ“ "What's the data retention policy - how far back should historical data go?"
```

### Skill Selection Logic

**How Claude Chooses**:

```
1. Parse user request
   â†“
2. **FIRST: Check if /interview needed** (CRITICAL)
   - New feature request? â†’ /interview FIRST
   - Architecture improvement? â†’ /interview FIRST
   - Refactoring with unclear scope? â†’ /interview FIRST
   - Requirements ambiguous? â†’ /interview FIRST
   â†“
3. Identify task type:
   - "implement feature" â†’ /interview â†’ plan mode â†’ /plan-implement-verify
   - "review PR" â†’ code-review:code-review
   - "analyze code" â†’ /sc:analyze OR /validate-architecture
   - "fix bug" â†’ /sc:troubleshoot (or /interview if requirements unclear)
   - "create spreadsheet" â†’ document-skills:xlsx
   - "research library" â†’ /sc:research (with Context7 MCP)
   â†“
4. Check context:
   - Is this part of larger workflow? â†’ Use custom workflow skill
   - Is this standalone task? â†’ Use SuperClaude or plugin skill
   â†“
5. Execute skill automatically (no explicit invocation needed)
```

**Priority Order** (when multiple skills match):

**TIER 0 (HIGHEST PRIORITY - Runs BEFORE everything else)**:
- **`/interview`** - Requirements discovery for new features/architecture
  - Triggers automatically for: new features, architecture changes, refactoring
  - **ALWAYS runs before planning or implementation**
  - Outputs: comprehensive specification document
  - Action: Enters plan mode after completion

**TIER 1 (Custom User Skills)** - Project-specific workflows:
- **`/plan-implement-verify`** - Full development cycle (after /interview)
- **`/worktree-setup`** - Git worktree creation
- **`/validate-architecture`** - Architecture validation
- **`/commit-push-pr`** - Git workflow automation

**TIER 2 (SuperClaude Skills)** - Intelligent orchestration:
- `/sc:implement`, `/sc:design`, `/sc:analyze`, `/sc:research`, etc.

**TIER 3 (Plugin Skills)** - Specialized capabilities:
- `code-review:code-review`, `document-skills:*`, `llm-application-dev:*`, etc.

**Example Automatic Selection**:

```
User: "Implement a dashboard with metrics visualization"
â†’ STEP 1: /interview (TIER 0 - runs FIRST)
   â†’ Deep questions about metrics, refresh rates, user workflows, etc.
   â†’ Output: dashboard-spec.md
â†’ STEP 2: Enter plan mode automatically
â†’ STEP 3: /plan-implement-verify (TIER 1)
   â†’ Within workflow: Uses /sc:design for architecture, /sc:implement for code

User: "Refactor authentication system for better scalability"
â†’ STEP 1: /interview (TIER 0 - architecture improvement)
   â†’ Questions about current bottlenecks, scale requirements, auth patterns
   â†’ Output: auth-refactor-spec.md
â†’ STEP 2: Enter plan mode
â†’ STEP 3: Execute refactoring workflow

User: "Review the PR for authentication changes"
â†’ Auto-selects: code-review:code-review (TIER 3 plugin)
â†’ No interview needed (not new feature, just review)

User: "Analyze this component for performance issues"
â†’ Auto-selects: /sc:analyze (TIER 2 SuperClaude)
â†’ May also use: /validate-architecture for architecture issues
â†’ No interview needed (analysis task, not implementation)

User: "Create a report spreadsheet with sales data"
â†’ Auto-selects: document-skills:xlsx (TIER 3 plugin)
â†’ No interview needed (straightforward document generation)

User: "Research best practices for React Server Components"
â†’ Auto-selects: /sc:research (TIER 2 SuperClaude)
â†’ Auto-uses: Context7 MCP for documentation
â†’ No interview needed (research task, not implementation)

User: "Build a RAG system with vector search"
â†’ STEP 1: /interview (TIER 0 - new feature)
   â†’ Questions about data sources, embedding models, retrieval strategy
   â†’ Output: rag-system-spec.md
â†’ STEP 2: Enter plan mode
â†’ STEP 3: llm-application-dev:ai-engineer (TIER 3 plugin)
   â†’ Auto-uses: llm-application-dev:vector-database-engineer
   â†’ Auto-uses: langchain-docs MCP for documentation
â†’ Result: Complete RAG implementation

User: "Fix login bug - users can't sign in"
â†’ Auto-selects: /sc:troubleshoot (TIER 2 SuperClaude)
â†’ No interview needed (clear bug fix, not new feature)

User: "Fix authentication - it's not working properly but unclear why"
â†’ STEP 1: /interview (TIER 0 - unclear requirements)
   â†’ Questions about symptoms, expected behavior, edge cases
   â†’ Output: auth-bug-spec.md
â†’ STEP 2: /sc:troubleshoot with full context
```

**Override Behavior**:
If you want to disable automatic invocation for a specific task, explicitly say:
- "Don't use any slash commands, just do X manually"
- "Skip the normal workflow and directly Y"

---

### Subagent Parallel Execution Mechanics

**HOW Parallel Execution Works:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Main Agent (You)                                           â”‚
â”‚  Task: "Implement dashboard feature"                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Spawn ALL Subagents in SINGLE Message (Parallel Launch)   â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚
â”‚  Task tool call #1: "Create types in src/app/types/..."    â”‚
â”‚  Task tool call #2: "Create component in src/app/..."      â”‚
â”‚  Task tool call #3: "Create hooks in src/app/hooks/..."    â”‚
â”‚  Task tool call #4: "Add mock data in src/app/data/..."    â”‚
â”‚  Task tool call #5: "Update App.tsx routing..."            â”‚
â”‚  [ALL 5 calls in ONE message - NOT sequential messages]    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Claude Code Orchestrator                                   â”‚
â”‚  Launches 5 subagents SIMULTANEOUSLY in background          â”‚
â””â”€â”¬â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  â”‚      â”‚      â”‚      â”‚      â”‚
  â–¼      â–¼      â–¼      â–¼      â–¼
â”Œâ”€â”€â”€â”  â”Œâ”€â”€â”€â”  â”Œâ”€â”€â”€â”  â”Œâ”€â”€â”€â”  â”Œâ”€â”€â”€â”
â”‚ 1 â”‚  â”‚ 2 â”‚  â”‚ 3 â”‚  â”‚ 4 â”‚  â”‚ 5 â”‚  â† Subagents (separate processes)
â””â”€â”¬â”€â”˜  â””â”€â”¬â”€â”˜  â””â”€â”¬â”€â”˜  â””â”€â”¬â”€â”˜  â””â”€â”¬â”€â”˜
  â”‚      â”‚      â”‚      â”‚      â”‚     â† Each runs independently
  â”‚      â”‚      â”‚      â”‚      â”‚     â† Each has PostToolUse validation
  â”‚      â”‚      â”‚      â”‚      â”‚     â† Total time = MAX(duration of slowest)
  â–¼      â–¼      â–¼      â–¼      â–¼
Done   Done   Done   Done   Done
  â”‚      â”‚      â”‚      â”‚      â”‚
  â””â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Main Agent Receives ALL Results                            â”‚
â”‚  Duration: ~15 min (not 5Ã—15min = 75min!)                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Key Mechanisms:**

1. **Single Message Launch** (CRITICAL):
   ```
   âœ“ CORRECT (Parallel):
   [Message with 5 Task tool calls] â†’ All 5 run simultaneously

   âœ— WRONG (Sequential):
   [Message with Task #1] â†’ wait â†’ [Message with Task #2] â†’ wait...
   ```

2. **Background Execution**:
   - Each subagent runs in a separate process/thread
   - Main agent doesn't wait sequentially
   - Total time = duration of the SLOWEST subagent (not sum of all)

3. **run_in_background: true** (Optional):
   - Can explicitly set `run_in_background: true` on Task tool
   - Use `TaskOutput` to retrieve results later
   - Useful for very long-running tasks (>30 min)

4. **No Dependencies Between Subagents**:
   - Each subagent must work independently
   - No subagent should depend on another's output
   - If dependencies exist, run sequentially instead

---

## Quick Start Guide

### New Session Initialization

**EVERY session starts in plan mode** unless explicitly instructed otherwise.

```bash
# 1. Check git status
git status

# 2. Review active worktrees (if any)
git worktree list

# 3. Review recent workflow performance
cat ~/.claude/metrics/workflow-success.json | jq '.overallSuccessRate'

# 4. Check for improvement suggestions
cat ~/.claude/metrics/improvement-suggestions.md
```

### Essential Commands

| Command | Purpose | When to Use |
|---------|---------|-------------|
| **`/interview`** | **Deep requirements discovery** | **FIRST for new features/architecture (automatic)** |
| `/worktree-setup [name]` | Create feature worktree | Start of any feature/bugfix |
| `/plan-implement-verify` | Full feature cycle | Feature implementation (after interview) |
| `/validate-architecture` | Check code quality | Before committing |
| `/commit-push-pr` | Complete git workflow | After validation passes |
| `/feedback-capture` | Record metrics | End of workflow |

### Default Development Flow (New Features)

```
Interview â†’ Plan â†’ Worktree â†’ Implement (Subagents) â†’ Validate â†’ Commit-Push-PR â†’ Feedback
    â†“        â†“         â†“            â†“                    â†“              â†“              â†“
  Deep    Think    Create     Parallel Execution   PostToolUse    Git Workflow   Metrics
Questions  Mode    Branch      (5-8 subagents)      Validation     Automation    Collection
  + Spec
```

### Simplified Flow (Bug Fixes / Simple Tasks)

```
Plan â†’ Worktree â†’ Implement â†’ Validate â†’ Commit-Push-PR â†’ Feedback
  â†“         â†“            â†“         â†“              â†“              â†“
Think    Create      Execute  PostToolUse    Git Workflow   Metrics
Mode    Branch                Validation     Automation    Collection
```

---

## Core Workflows

### WORKFLOW 1: Feature Implementation (Plan-Implement-Verify)

**Trigger**: New feature request
**Duration**: 30-60 minutes
**Success Rate**: Target 90%+
**Command**: `/plan-implement-verify`

#### Phase 1: Planning (10-15 min)

**Tool Sequence**:
```
1. Grep: Search for similar components/patterns
   â†’ pattern: related component names, type definitions
   â†’ scope: src/app/components/, src/app/types/

2. Read: Examine 3-5 reference files
   â†’ files: similar components, integration points, type definitions
   â†’ goal: understand existing patterns

3. Serena: get_symbols_overview (if available)
   â†’ file: reference component
   â†’ depth: 1 (get methods/structure)

4. Think: Design solution architecture
   â†’ considerations:
     - Component structure (composition pattern preferred)
     - Type definitions needed
     - Integration with existing code
     - State management approach
     - Data flow patterns

5. Break down into 5-8 granular tasks
   â†’ each task: <15 minutes, single file/module
   â†’ example tasks:
     1. Define TypeScript interfaces
     2. Create main component structure
     3. Implement business logic hooks
     4. Create sub-components
     5. Add mock data integration
     6. Update routing/navigation
     7. Add styling
     8. Integration testing

6. AskUserQuestion: Get approval on approach
```

#### Phase 2: Worktree Setup (2-3 min)

**Tool Sequence**:
```bash
# Command: /worktree-setup [feature-name]

1. Bash: Ensure clean main branch
   â†’ cd /Users/jaewookim/Desktop/Personal\ Coding/AI\ Audit
   â†’ git checkout main
   â†’ git pull origin main

2. Bash: Create feature worktree
   â†’ git worktree add worktrees/feature-[name] -b feature/[name]

3. Bash: Navigate to worktree
   â†’ cd worktrees/feature-[name]

4. Bash: Verify setup
   â†’ git worktree list
   â†’ git status
```

#### Phase 3: Parallel Implementation (20-40 min)

**Subagent Orchestration** (SPAWN ALL IN PARALLEL):

```
Main Agent: Architecture oversight
â”œâ”€ Subagent 1: Type Definitions
â”‚  Task: "Create TypeScript interfaces for [feature] in src/app/types/[feature].ts"
â”‚  Expected: Interface definitions with proper exports
â”‚  PostToolUse: Type check validation
â”‚  Duration: 5-10 min
â”‚
â”œâ”€ Subagent 2: Main Component
â”‚  Task: "Create [FeatureName].tsx in src/app/components/[feature]/ with props and structure"
â”‚  Expected: React component with TypeScript props
â”‚  PostToolUse: Component syntax validation
â”‚  Duration: 10-15 min
â”‚
â”œâ”€ Subagent 3: Sub-Components
â”‚  Task: "Create sub-components for [feature] ensuring reusability"
â”‚  Expected: Reusable child components
â”‚  PostToolUse: Component validation
â”‚  Duration: 10-15 min
â”‚
â”œâ”€ Subagent 4: Business Logic
â”‚  Task: "Implement custom hooks for [feature] (use[Feature].ts)"
â”‚  Expected: React hooks with type safety
â”‚  PostToolUse: Logic validation
â”‚  Duration: 10-15 min
â”‚
â”œâ”€ Subagent 5: Data Integration
â”‚  Task: "Add mock data for [feature] in src/app/data/mock[Feature].ts"
â”‚  Expected: Mock data matching types
â”‚  PostToolUse: Data structure validation
â”‚  Duration: 5-10 min
â”‚
â”œâ”€ Subagent 6: Integration (Optional)
â”‚  Task: "Update App.tsx routing to include [feature]"
â”‚  Expected: Navigation integration
â”‚  PostToolUse: Integration validation
â”‚  Duration: 5-10 min
â”‚
â””â”€ Subagent 7: Styling (Optional)
   Task: "Add Tailwind styling to [feature] components"
   Expected: Responsive, theme-consistent styling
   PostToolUse: Style validation
   Duration: 5-10 min
```

**Spawn Command**:
```bash
# CRITICAL: Launch ALL subagents in SINGLE message (parallel execution)
/subagent-spawn "Create TypeScript interfaces for [feature]..."
/subagent-spawn "Create [FeatureName].tsx component..."
/subagent-spawn "Create sub-components..."
/subagent-spawn "Implement business logic hooks..."
/subagent-spawn "Add mock data..."
/subagent-spawn "Update App.tsx routing..."
```

#### Phase 4: Integration & Validation (10-15 min)

**Tool Sequence**:
```bash
1. Bash: Type checking
   â†’ cd frontend && npx tsc --noEmit
   â†’ Expected: No type errors

2. Bash: Build validation
   â†’ npm run build
   â†’ Expected: Successful build

3. Bash: Start dev server (manual testing)
   â†’ npm run dev &
   â†’ Action: Test in browser at localhost:5173

4. Manual Checklist:
   - [ ] Component renders without errors
   - [ ] Props typed correctly
   - [ ] State management works
   - [ ] Data flows correctly
   - [ ] Styling matches design system
   - [ ] Responsive on mobile viewport
   - [ ] No console errors in browser
   - [ ] Navigation/routing works

5. Command: /validate-architecture
   â†’ Runs: ~/.claude/hooks/architecture-analyzer.js
   â†’ Checks: Unnecessary lines, OOP principles, file size, code quality
   â†’ Expected: No errors, warnings acceptable

6. Read: Review all changed files
   â†’ Bash: git diff --stat
   â†’ Bash: git diff
   â†’ Verification: All changes intentional
```

#### Phase 5: Commit-Push-PR (5-10 min)

**Tool Sequence**:
```bash
# Command: /commit-push-pr

1. Bash: Stage changes
   â†’ git add -A

2. Create commit message (use template):
   feat: Add [feature] with [key capabilities]

   Detailed explanation:
   - Created [components] with [functionality]
   - Added [types] for type safety
   - Implemented [logic] using [pattern]

   Technical details:
   - Architecture: Composition pattern for reusability
   - Components: [list components]
   - Types: [list types]
   - OOP: Single Responsibility Principle applied

   Testing:
   - Type check: âœ“
   - Build: âœ“
   - Architecture validation: âœ“
   - Manual testing: âœ“

   Related:
   - Workflow: plan-implement-verify

   Co-authored-by: Claude Sonnet 4.5 <noreply@anthropic.com>

3. Bash: Commit
   â†’ git commit -m "[paste commit message]"

4. Bash: Push to remote
   â†’ git push -u origin feature/[name]

5. Bash: Create PR (using gh CLI)
   â†’ gh pr create --title "feat: [Feature]" --body "[PR description]"
```

#### Phase 6: Feedback Loop (2-3 min)

**Tool Sequence**:
```bash
# Command: /feedback-capture plan-implement-verify success "notes"

1. Capture metrics
   â†’ Updates: ~/.claude/metrics/workflow-success.json
   â†’ Updates: ~/.claude/metrics/tool-usage.json
   â†’ Duration recorded, success rate updated

2. Analyze for patterns
   â†’ Check: Bottlenecks detected?
   â†’ Check: Auto-update triggers met?
   â†’ Record: Tool usage statistics

3. Generate improvements (if applicable)
   â†’ Updates: ~/.claude/metrics/improvement-suggestions.md
```

---

### WORKFLOW 2: Bug Fix (Fast Track)

**Trigger**: Bug report or error
**Duration**: 10-20 minutes
**Success Rate**: Target 85%+

#### Tool Sequence

```
PHASE 1: Diagnosis (5 min)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
1. Grep: Find error patterns
   â†’ pattern: error message, stack trace keywords
   â†’ scope: relevant modules

2. Read: Examine relevant files (max 3)
   â†’ files: where error occurs, related functions

3. Bash: Check git history
   â†’ git log --oneline --all --grep="[keyword]" -10
   â†’ goal: find recent related changes

4. Think: Root cause analysis
   â†’ identify: actual cause vs symptom
   â†’ plan: minimal fix required

PHASE 2: Worktree Setup (2 min)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
5. Command: /worktree-setup bugfix-[issue-id]
   â†’ creates: worktrees/bugfix-[issue-id]

PHASE 3: Fix Implementation (5-10 min)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
6. Spawn Subagent 1: Fix primary issue
   â†’ task: specific fix in specific file
   â†’ PostToolUse: Type check + architecture validation

7. Spawn Subagent 2 (optional): Add test/documentation
   â†’ task: prevent regression
   â†’ PostToolUse: Test execution

PHASE 4: Validation (3 min)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
8. Command: /validate-architecture
   â†’ verify: fix doesn't introduce new issues

9. Manual testing:
   â†’ reproduce: original bug scenario
   â†’ verify: bug is fixed
   â†’ check: no side effects

PHASE 5: Commit-Push-PR (3 min)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
10. Command: /commit-push-pr
    â†’ commit type: "fix:"
    â†’ PR description: includes steps to reproduce + fix explanation

PHASE 6: Feedback (1 min)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
11. Command: /feedback-capture bug-fix success "Issue: [#], Duration: [X]min"
```

---

### WORKFLOW 3: Refactoring (Architecture Improvement)

**Trigger**: Code quality improvement needed
**Duration**: 40-90 minutes
**Success Rate**: Target 80%+

#### Tool Sequence

```
PHASE 1: Analysis (15 min)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
1. Read: Current implementation
   â†’ files: target files for refactoring

2. Command: /validate-architecture
   â†’ analyze: current issues
   â†’ review: ~/.claude/metrics/architecture-report.json
   â†’ identify: violations of clean code, OOP principles

3. Grep: Find code duplication
   â†’ pattern: repeated code blocks
   â†’ scope: module being refactored

4. Think: Design clean architecture
   â†’ apply: SOLID principles
   â†’ plan: composition over inheritance
   â†’ ensure: no unnecessary lines

5. Break down: refactoring steps
   â†’ task per file/module
   â†’ ensure: each step testable

PHASE 2: Worktree Setup (2 min)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
6. Command: /worktree-setup refactor-[area]

PHASE 3: Refactoring (30-60 min)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
7. Spawn multiple subagents (4-8):
   â†’ each: refactor one file/module
   â†’ ensure: extract reusable logic (DRY)
   â†’ apply: proper OOP patterns
   â†’ PostToolUse: validates each change

PHASE 4: Before/After Comparison (10 min)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
8. Command: /validate-architecture
   â†’ capture: "before" report
   â†’ compare: improvements
   â†’ verify: issues resolved

9. Bash: Run full test suite
   â†’ ensure: no functionality broken

10. Manual testing:
    â†’ verify: all features still work

PHASE 5: Commit-Push-PR (5 min)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
11. Command: /commit-push-pr
    â†’ commit type: "refactor:"
    â†’ PR description: architecture improvements documented

PHASE 6: Feedback (2 min)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
12. Command: /feedback-capture refactoring success "Improved: [metrics]"
```

---

### WORKFLOW 4: Ralph-Wiggum Loop Prevention

**Trigger**: Long-running task (>10 tool calls without progress)
**Purpose**: Prevent infinite loops and stuck states

#### Auto-Detection

The **ralph-wiggum plugin** automatically detects loops:
- **10+ tool calls** with same pattern
- **Repetitive actions** without progress
- **Escalating retries** on same failure

#### Recovery Sequence

```
STEP 1: Auto-Checkpoint
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â†’ Ralph-wiggum detects loop
â†’ Automatically saves current state
â†’ Pauses execution

STEP 2: Analysis
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
1. Review last 10 tool calls
   â†’ identify: repeating pattern
   â†’ analyze: why stuck

2. Think: Alternative approach
   â†’ question: is task too complex?
   â†’ consider: different strategy
   â†’ evaluate: need to break down further

STEP 3: Decision
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Option A: Simplify approach
â†’ reduce scope
â†’ try different method
â†’ resume with new strategy

Option B: Delegate to subagent
â†’ spawn subagent for blocked subtask
â†’ subagent uses fresh perspective
â†’ PostToolUse catches issues

Option C: Ask user for clarification
â†’ AskUserQuestion: explain blocker
â†’ get: user guidance
â†’ resume with clarification

STEP 4: Resume
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â†’ Apply chosen strategy
â†’ Monitor for loop recurrence
â†’ Capture in feedback: bottleneck identified

STEP 5: Feedback
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â†’ Record in: ~/.claude/metrics/bottlenecks.json
â†’ Suggest: workflow improvement if pattern common
```

---

## Subagent Orchestration

### Core Principles

1. **Granular Tasks**: Each subagent = one file/module, <15 min
2. **Parallel Execution**: Launch all subagents in SINGLE message
3. **PostToolUse Validation**: Every subagent change validated automatically
4. **Clear Deliverables**: Exact file path and expected output specified
5. **No Dependencies**: Subagents work independently

### Orchestration Pattern 1: Component Development

```
Main Agent (Oversees architecture)
â”‚
â”œâ”€ Subagent 1: Types (src/app/types/[feature].ts)
â”‚  â”œâ”€ Define interfaces
â”‚  â”œâ”€ Export all types
â”‚  â””â”€ PostToolUse: Type check âœ“
â”‚
â”œâ”€ Subagent 2: Main Component (src/app/components/[Feature].tsx)
â”‚  â”œâ”€ Component structure
â”‚  â”œâ”€ Props with types
â”‚  â””â”€ PostToolUse: Component validation âœ“
â”‚
â”œâ”€ Subagent 3: Sub-Components (src/app/components/[feature]/*)
â”‚  â”œâ”€ Reusable children
â”‚  â”œâ”€ Composition pattern
â”‚  â””â”€ PostToolUse: Component validation âœ“
â”‚
â”œâ”€ Subagent 4: Logic (src/app/hooks/use[Feature].ts)
â”‚  â”œâ”€ Custom React hooks
â”‚  â”œâ”€ State management
â”‚  â””â”€ PostToolUse: Logic validation âœ“
â”‚
â””â”€ Subagent 5: Data (src/app/data/mock[Feature].ts)
   â”œâ”€ Mock data
   â”œâ”€ Matches type definitions
   â””â”€ PostToolUse: Data validation âœ“
```

### Orchestration Pattern 2: Testing Pipeline

```
Main Agent (Test orchestration)
â”‚
â”œâ”€ Subagent 1: Unit Tests
â”‚  â””â”€ Individual function tests
â”‚
â”œâ”€ Subagent 2: Integration Tests
â”‚  â””â”€ Component integration tests
â”‚
â”œâ”€ Subagent 3: E2E Scenarios
â”‚  â””â”€ User flow tests
â”‚
â””â”€ Subagent 4: Test Documentation
   â””â”€ Test coverage report
```

### Orchestration Pattern 3: Code Quality Enforcement

```
Main Agent (Quality gate validation)
â”‚
â”œâ”€ Subagent 1: Type Checking
â”‚  â””â”€ npx tsc --noEmit
â”‚
â”œâ”€ Subagent 2: Lint Validation
â”‚  â””â”€ npx eslint (if configured)
â”‚
â”œâ”€ Subagent 3: Architecture Analysis
â”‚  â””â”€ ~/.claude/hooks/architecture-analyzer.js
â”‚
â””â”€ Subagent 4: Performance Check
   â””â”€ Bundle size, render performance
```

### Spawn Best Practices

**DO**:
```bash
# âœ“ Specific task with file path
/subagent-spawn "Create MetricCard.tsx in src/app/components/dashboard/ with props: metric (DashboardMetric), onClick, className"

# âœ“ Clear success criteria
/subagent-spawn "Add 5 sample dashboard metrics to src/app/data/mockDashboard.ts matching DashboardMetric interface"

# âœ“ Single responsibility
/subagent-spawn "Fix task status update logic in src/app/hooks/useTaskManager.ts line 45-60"
```

**DON'T**:
```bash
# âœ— Too vague
/subagent-spawn "Improve dashboard"

# âœ— Multiple responsibilities
/subagent-spawn "Create component, add types, integrate with app, and test"

# âœ— No file path
/subagent-spawn "Add some metrics"
```

---

## Global Infrastructure

### PostToolUse Hook

**Location**: `~/.claude/hooks/post-tool-use.sh`
**Purpose**: Automatic validation after every file modification
**Execution**: Auto-triggered by subagents when `strictMode: true`

#### Validation Sequence

```
1. Type Check (if TypeScript)
   â†’ npx tsc --noEmit
   â†’ Exit 1 if errors (strict mode)

2. Build Validation (if skipBuild: false)
   â†’ npm run build
   â†’ Exit 1 if fails (strict mode)

3. Lint Check (if .eslintrc exists)
   â†’ npx eslint
   â†’ Exit 1 if errors (strict mode)

4. Architecture Analysis
   â†’ node ~/.claude/hooks/architecture-analyzer.js
   â†’ Checks:
     * Unnecessary lines
     * File size (<800 lines)
     * Function size (<50 lines)
     * Forbidden patterns (any, console.log, debugger)
     * OOP principles (God classes, public fields)
     * Code duplication
   â†’ Exit 1 if errors (strict mode)

5. Report Generation
   â†’ ~/.claude/metrics/last-validation-report.json
   â†’ ~/.claude/metrics/architecture-report.json (if issues)
```

#### Configuration

**File**: `~/.claude/hooks/hook-config.json`

```json
{
  "skipBuild": false,          // Set true to skip build (faster, less thorough)
  "skipLint": true,            // Set false to enable lint checking
  "strictMode": true,          // Fail on errors (recommended)
  "architectureRules": {
    "maxFileLines": 800,
    "maxFunctionLines": 50,
    "enforceOOP": true,
    "enforceCleanCode": true
  }
}
```

---

### Slash Commands

**Location**: `~/.claude/commands/*.md`
**Purpose**: Reusable workflow automation

| Command | File | Purpose |
|---------|------|---------|
| `/worktree-setup` | `worktree-setup.md` | Create isolated feature worktree |
| `/plan-implement-verify` | `plan-implement-verify.md` | Full feature development cycle |
| `/validate-architecture` | `validate-architecture.md` | Deep code quality analysis |
| `/commit-push-pr` | `commit-push-pr.md` | Complete git workflow |
| `/subagent-spawn` | `subagent-spawn.md` | Launch focused subagent |
| `/feedback-capture` | `feedback-capture.md` | Record workflow metrics |

**Usage**:
```bash
# Direct invocation
/worktree-setup dashboard-metrics

# Nested in workflows
# /plan-implement-verify includes /worktree-setup, /validate-architecture, /commit-push-pr, /feedback-capture
```

---

### Metrics System

**Location**: `~/.claude/metrics/`
**Purpose**: Continuous improvement through feedback loops

#### Files

1. **workflow-success.json**
   - Tracks: success rate, duration, failure points per workflow
   - Updated by: `/feedback-capture`
   - Used for: Identifying reliable vs problematic workflows

2. **tool-usage.json**
   - Tracks: tool usage frequency, duration, success rate
   - Updated by: automatic tool call tracking
   - Used for: Identifying underutilized or overused tools

3. **bottlenecks.json**
   - Tracks: recurring issues, delays, blockers
   - Updated by: failure analysis
   - Used for: Prioritizing workflow improvements

4. **improvement-suggestions.md**
   - Tracks: auto-generated improvement suggestions
   - Updated by: metrics analysis (weekly or every 10 workflows)
   - Used for: Guiding CLAUDE.md updates

#### Auto-Update Triggers

| Trigger | Condition | Action |
|---------|-----------|--------|
| **Failure Pattern** | 3+ workflows fail at same step | Add warning to CLAUDE.md |
| **Unused Tool** | Tool not used in 20+ workflows | Mark as deprecated |
| **New Pattern** | Same approach used 5+ times | Formalize into workflow |
| **Subagent Timeout** | 5+ subagents exceed 15 min | Update task granularity guidance |
| **Architecture Issues** | Same rule violated 3+ times | Add specific rule explanation |

---

## MCP Integration

### Available MCP Servers

The following MCP (Model Context Protocol) servers are available and **automatically used** when appropriate:

**Quick Reference**:
1. **Context7** - General library/framework documentation (React, Next.js, etc.)
2. **Greptile** - GitHub PR review and code analysis
3. **Serena** - Semantic code intelligence (symbol-based navigation)
4. **Sequential Thinking** - Complex problem solving with reasoning chains
5. **LangChain Docs** - LangChain/LangGraph documentation for LLM apps
6. **Playwright** - Browser automation and testing

---

#### 1. Context7 (Documentation Lookup)

**Auto-Use Triggers**:
- User asks about library/framework usage
- Need up-to-date documentation for specific version
- Looking up API reference or examples

**Example Scenarios**:
```
User: "How do I use React Server Components?"
â†’ Auto-uses: mcp__plugin_context7_context7__query-docs
â†’ Returns: Latest React docs with examples

User: "What's the syntax for Next.js 15 route handlers?"
â†’ Auto-uses: mcp__plugin_context7_context7__resolve-library-id + query-docs
â†’ Returns: Next.js 15 specific documentation
```

**Tool Sequence (Automatic)**:
```
1. mcp__plugin_context7_context7__resolve-library-id
   â†’ Resolves library name to Context7 ID (e.g., "/vercel/next.js")

2. mcp__plugin_context7_context7__query-docs
   â†’ Fetches documentation and code examples
   â†’ Returns: Markdown with syntax, examples, best practices
```

**Manual Override**:
If you don't want Context7 to be used for a specific question, say:
- "Don't look up docs, just use your training knowledge"
- "Answer from memory only"

---

#### 2. Greptile (Code Review & PR Analysis)

**Auto-Use Triggers**:
- User mentions "pull request", "PR", "code review"
- Analyzing GitHub repository code
- Searching for patterns across large codebase

**Example Scenarios**:
```
User: "Review the open PR for the dashboard feature"
â†’ Auto-uses: mcp__plugin_greptile_greptile__list_pull_requests
â†’ Auto-uses: mcp__plugin_greptile_greptile__get_merge_request
â†’ Auto-uses: mcp__plugin_greptile_greptile__list_merge_request_comments
â†’ Returns: PR summary, comments, review status

User: "What are common issues in Greptile review comments?"
â†’ Auto-uses: mcp__plugin_greptile_greptile__search_greptile_comments
â†’ Returns: Pattern analysis of review feedback
```

**Tool Sequence (Automatic)**:
```
1. mcp__plugin_greptile_greptile__list_pull_requests
   â†’ Get list of PRs (by branch, author, or state)

2. mcp__plugin_greptile_greptile__get_merge_request
   â†’ Get detailed PR info (metadata, stats, review analysis)

3. mcp__plugin_greptile_greptile__list_merge_request_comments
   â†’ Get all comments (Greptile reviews + human comments)

4. mcp__plugin_greptile_greptile__trigger_code_review (if needed)
   â†’ Trigger new Greptile review
```

**Custom Context**:
```
5. mcp__plugin_greptile_greptile__list_custom_context
   â†’ Get organization-specific patterns and instructions

6. mcp__plugin_greptile_greptile__create_custom_context
   â†’ Add new custom context (project-specific rules)
```

---

#### 3. Serena (Semantic Code Intelligence)

**Auto-Use Triggers**:
- Exploring codebase structure or understanding code
- Symbol-based operations (find classes, methods, functions)
- Code navigation and dependency analysis
- Refactoring that requires understanding code relationships

**Example Scenarios**:
```
User: "Find all references to UserAuth class"
â†’ Auto-uses: mcp__serena__find_symbol (name_path_pattern="UserAuth")
â†’ Auto-uses: mcp__serena__find_referencing_symbols
â†’ Returns: All locations where UserAuth is used

User: "What methods does the Dashboard component have?"
â†’ Auto-uses: mcp__serena__get_symbols_overview (depth=1)
â†’ Returns: Symbol tree with all methods

User: "Rename validateUser to validateUserCredentials"
â†’ Auto-uses: mcp__serena__rename_symbol
â†’ Updates: All references throughout codebase
```

**Tool Sequence (Automatic)**:
```
1. mcp__serena__list_dir
   â†’ List files and directories (with recursion)

2. mcp__serena__find_file
   â†’ Find files matching patterns

3. mcp__serena__get_symbols_overview
   â†’ Get high-level symbol tree (classes, methods, functions)

4. mcp__serena__find_symbol
   â†’ Find specific symbols by name path pattern
   â†’ Supports substring matching, depth control

5. mcp__serena__find_referencing_symbols
   â†’ Find all references to a symbol

6. mcp__serena__search_for_pattern
   â†’ Flexible regex search across codebase
```

**Symbolic Editing** (Architecture-Aware):
```
7. mcp__serena__replace_symbol_body
   â†’ Replace entire symbol definition (method, class, function)

8. mcp__serena__insert_after_symbol / insert_before_symbol
   â†’ Insert code at specific symbol locations

9. mcp__serena__rename_symbol
   â†’ Rename symbol throughout entire codebase
```

**Why Serena Over Basic Tools**:
- **Token-efficient**: Read only necessary symbols, not entire files
- **Architecture-aware**: Understands code structure (classes, methods, inheritance)
- **Precise editing**: Symbol-level changes ensure correct placement
- **Dependency tracking**: Find all references automatically

**When to Use Serena vs. Basic Tools**:
```
âœ“ Use Serena:
- Understanding code structure (classes, methods, functions)
- Finding symbols by name or pattern
- Refactoring (rename, move, extract)
- Dependency analysis (what uses this class?)

âœ“ Use Basic Tools (Read, Grep, Edit):
- Reading non-code files (markdown, JSON, config)
- Simple text search across all file types
- Small edits within a symbol (few lines)
```

---

#### 4. Sequential Thinking (Complex Problem Solving)

**Auto-Use Triggers**:
- Complex multi-step problem requiring deep reasoning
- Debugging tricky issues with multiple hypotheses
- Architectural decisions with trade-offs
- Planning complex features with many unknowns

**Example Scenarios**:
```
User: "Why is my React component re-rendering infinitely?"
â†’ Auto-uses: mcp__sequential-thinking__sequentialthinking
â†’ Process:
  Thought 1: Analyze component dependencies
  Thought 2: Check useEffect dependencies array
  Thought 3: Verify state update patterns
  Thought 4: Generate hypothesis (missing dependency)
  Thought 5: Verify hypothesis by reading code
  Thought 6: Confirm root cause
â†’ Returns: Definitive answer with reasoning chain

User: "Design a scalable architecture for real-time notifications"
â†’ Auto-uses: Sequential Thinking for multi-faceted analysis
â†’ Process: Considers WebSockets, SSE, polling, trade-offs, scales
â†’ Returns: Comprehensive architecture design with justification
```

**How Sequential Thinking Works**:
```
1. Problem presented
   â†“
2. Break down into thoughts (initially estimate 5-10 thoughts)
   â†“
3. For each thought:
   - Analyze current understanding
   - Generate hypothesis
   - Verify hypothesis (may use other tools: Read, Grep, etc.)
   - Revise if hypothesis wrong (thoughtNumber can go beyond initial estimate)
   â†“
4. Branching if needed:
   - Explore alternative approaches
   - Compare trade-offs
   â†“
5. Converge to solution
   â†“
6. Return final answer with full reasoning chain
```

**Key Features**:
- **Adaptive**: Can add more thoughts if problem is harder than expected
- **Self-correcting**: Can revise previous thoughts if wrong
- **Branching**: Can explore multiple approaches in parallel
- **Transparent**: Shows full reasoning chain to user

**When to Use Sequential Thinking**:
```
âœ“ Use Sequential Thinking:
- Debugging complex issues (root cause unclear)
- Architectural decisions (multiple valid approaches)
- Performance optimization (need to analyze bottlenecks)
- Feature design (many unknowns, need to explore)

âœ— Don't Use Sequential Thinking:
- Simple, straightforward tasks
- Tasks with clear single path
- When speed is critical (adds overhead)
```

---

#### 5. LangChain Docs (LLM Framework Documentation)

**Auto-Use Triggers**:
- User asks about LangChain, LangGraph usage
- Building LLM applications, agents, RAG systems
- Need documentation for LangChain ecosystem

**Example Scenarios**:
```
User: "How do I create a LangGraph agent with memory?"
â†’ Auto-uses: mcp__langchain-docs__list_doc_sources
â†’ Auto-uses: mcp__langchain-docs__fetch_docs
â†’ Returns: LangGraph documentation with agent examples

User: "What's the best way to implement RAG with LangChain?"
â†’ Auto-uses: langchain-docs for RAG documentation
â†’ Returns: Step-by-step RAG implementation guide

User: "How do I use vector stores in LangChain?"
â†’ Auto-uses: langchain-docs for vector store docs
â†’ Returns: Vector store integration examples
```

**Tool Sequence (Automatic)**:
```
1. mcp__langchain-docs__list_doc_sources
   â†’ Get available documentation sources (LangChain, LangGraph)
   â†’ Returns: URLs to llms.txt files

2. mcp__langchain-docs__fetch_docs
   â†’ Fetch documentation from specific URL
   â†’ Returns: Markdown documentation with code examples
```

**Available Documentation**:
- **LangChain**: Core library for LLM applications
  - Chains, agents, memory, callbacks
  - Prompt templates, output parsers
  - Integrations (OpenAI, Anthropic, etc.)
- **LangGraph**: State machines for agent workflows
  - Graph-based agent orchestration
  - Cycles, conditionals, persistence
  - Human-in-the-loop patterns

**Use Cases**:
```
âœ“ Building chatbots with memory
âœ“ Creating RAG (Retrieval Augmented Generation) systems
âœ“ Implementing multi-agent systems
âœ“ Vector database integration (Pinecone, Weaviate, Chroma)
âœ“ Prompt engineering and optimization
âœ“ LLM chains and workflows
```

**Combine with Skills**:
```
User: "Build a RAG system with LangChain"
â†’ Auto-uses: langchain-docs MCP (get documentation)
â†’ Auto-uses: llm-application-dev:ai-engineer (implement RAG)
â†’ Auto-uses: llm-application-dev:vector-database-engineer (setup vector DB)
â†’ Result: Complete RAG implementation with best practices
```

---

#### 6. Playwright (Browser Automation)

**Auto-Use Triggers**:
- User mentions "browser", "scraping", "web page", "screenshot"
- UI testing or visual verification needed
- Automated form filling or interaction

**Example Scenarios**:
```
User: "Take a screenshot of localhost:5173"
â†’ Auto-uses: mcp__plugin_playwright_playwright__browser_navigate
â†’ Auto-uses: mcp__plugin_playwright_playwright__browser_take_screenshot
â†’ Returns: Screenshot file

User: "Test the login form on the staging site"
â†’ Auto-uses: browser_navigate â†’ browser_fill_form â†’ browser_click
â†’ Returns: Test results with screenshots
```

**Tool Sequence (Automatic)**:
```
1. mcp__plugin_playwright_playwright__browser_navigate
   â†’ Navigate to URL

2. mcp__plugin_playwright_playwright__browser_snapshot
   â†’ Capture accessibility snapshot (better than screenshot for actions)

3. mcp__plugin_playwright_playwright__browser_click / browser_fill_form
   â†’ Interact with page elements

4. mcp__plugin_playwright_playwright__browser_take_screenshot
   â†’ Visual verification (PNG/JPEG)

5. mcp__plugin_playwright_playwright__browser_evaluate
   â†’ Execute JavaScript in page context
```

**Advanced Operations**:
```
6. mcp__plugin_playwright_playwright__browser_run_code
   â†’ Execute complex Playwright scripts

7. mcp__plugin_playwright_playwright__browser_network_requests
   â†’ Monitor network traffic

8. mcp__plugin_playwright_playwright__browser_console_messages
   â†’ Capture console logs (errors, warnings, debug)
```

---

### MCP Usage Best Practices

1. **Trust Automatic Selection**:
   - Claude automatically chooses the right MCP server
   - No need to specify "use Context7" or "use Greptile"
   - Override only if automatic selection is wrong

2. **Combine with Other Tools**:
   ```
   Example workflow #1 (Frontend):
   1. Context7: Get latest React docs
   2. Read: Check existing component patterns
   3. Write: Create new component using learned patterns
   4. Playwright: Test component in browser

   Example workflow #2 (LLM Application):
   1. langchain-docs: Get LangGraph documentation
   2. llm-application-dev:ai-engineer: Design agent architecture
   3. llm-application-dev:vector-database-engineer: Setup vector store
   4. llm-application-dev:prompt-engineer: Optimize prompts
   5. Write: Implement RAG system
   6. Test: Verify semantic search accuracy
   ```

3. **MCP + Subagents**:
   - Subagents can use MCP tools independently
   - Example: Subagent 1 uses Context7 for docs while Subagent 2 uses Greptile for PR review

4. **Performance Considerations**:
   - Context7 queries: ~2-5 seconds
   - Greptile queries: ~3-10 seconds (depends on PR size)
   - langchain-docs: ~2-4 seconds (documentation fetch)
   - Playwright: ~5-15 seconds (depends on page complexity)
   - Serena symbol search: ~1-3 seconds (token-efficient)
   - Sequential Thinking: ~10-30 seconds (complex reasoning)
   - Factor into workflow timing estimates

---

### MCP Limitations & Fallbacks

**Context7**:
- Limitation: Only supports libraries indexed by Context7
- Fallback: Use WebSearch for unindexed libraries
- Alternative: Read local node_modules or docs directly

**Greptile**:
- Limitation: Requires GitHub repository connection
- Fallback: Use `gh` CLI via Bash tool
- Alternative: Manual `git log` and `git diff` analysis

**Playwright**:
- Limitation: Requires browser installation
- Fallback: Use `mcp__plugin_playwright_playwright__browser_install`
- Alternative: Manual testing with `npm run dev`

**LangChain Docs**:
- Limitation: Only supports LangChain and LangGraph documentation
- Fallback: Use Context7 for other LLM libraries (OpenAI, Anthropic SDK)
- Alternative: Use WebSearch for general LLM/AI documentation

**Serena**:
- Limitation: Requires LSP-compatible languages (Python, TypeScript, JavaScript, Java, etc.)
- Fallback: Use basic tools (Read, Grep, Edit) for unsupported languages
- Alternative: Read entire files if symbolic navigation not available

**Sequential Thinking**:
- Limitation: Adds overhead (~10-30 seconds) for simple tasks
- Fallback: Skip for straightforward tasks with clear path
- Alternative: Direct implementation for simple requests

---

## Testing Strategy

### Core Principles

**CRITICAL**: Testing is NOT optional. Every feature MUST have:
1. **High test coverage** (target: 80%+ overall, 90%+ for critical paths)
2. **Multiple testing levels** (unit, integration, E2E)
3. **Comprehensive assertions** (not just "it doesn't crash" - verify actual results)
4. **Edge case testing** (boundary conditions, error states, null/undefined)

---

### Testing Pyramid

```
              â•±â•²
             â•± E2E â•²               â† Few (10-20% of tests)
            â•±â”€â”€â”€â”€â”€â”€â”€â”€â•²                Slow, comprehensive user flows
           â•±â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•²
          â•± Integration â•²           â† Moderate (30-40% of tests)
         â•±â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•²             Medium speed, component interactions
        â•±â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•²
       â•±   Unit Tests     â•²         â† Many (50-60% of tests)
      â•±â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•²          Fast, isolated functions/components
```

---

### 1. Unit Tests (50-60% of tests)

**What to Test**:
- Individual functions (utilities, helpers, hooks)
- Component rendering (props, state changes)
- Business logic (calculations, validations, transformations)

**Test Structure**:
```typescript
describe('calculateMetricScore', () => {
  it('should return 0 for empty data', () => {
    const result = calculateMetricScore([]);
    expect(result).toBe(0);  // âœ“ Verify exact result value
  });

  it('should calculate average correctly', () => {
    const result = calculateMetricScore([10, 20, 30]);
    expect(result).toBe(20);  // âœ“ Verify exact calculation
  });

  it('should handle negative numbers', () => {
    const result = calculateMetricScore([-10, 10]);
    expect(result).toBe(0);  // âœ“ Verify edge case
  });

  it('should throw error for non-numeric input', () => {
    expect(() => calculateMetricScore(['invalid'])).toThrow();  // âœ“ Verify error handling
  });
});
```

**BAD Unit Tests** (avoid these):
```typescript
// âœ— Too vague - doesn't verify actual result
it('should work', () => {
  const result = calculateMetricScore([10, 20]);
  expect(result).toBeDefined();  // âœ— Meaningless assertion
});

// âœ— No edge cases
describe('formatDate', () => {
  it('should format date', () => {
    expect(formatDate('2024-01-01')).toBeTruthy();  // âœ— Doesn't verify format
  });
});
```

---

### 2. Integration Tests (30-40% of tests)

**What to Test**:
- Component interactions (parent-child communication)
- State management across components
- Data flow through hooks and context
- API integration (with mocked backend)

**Test Structure**:
```typescript
describe('DashboardMetrics integration', () => {
  it('should fetch metrics and display them correctly', async () => {
    // Setup: Mock API
    const mockMetrics = [
      { id: 1, name: 'Revenue', value: 10000 },
      { id: 2, name: 'Users', value: 500 }
    ];
    vi.spyOn(api, 'fetchMetrics').mockResolvedValue(mockMetrics);

    // Act: Render component
    render(<DashboardMetrics />);

    // Assert: Loading state
    expect(screen.getByText(/loading/i)).toBeInTheDocument();

    // Assert: Data displayed after fetch
    await waitFor(() => {
      expect(screen.getByText('Revenue')).toBeInTheDocument();
      expect(screen.getByText('10000')).toBeInTheDocument();  // âœ“ Verify actual values
      expect(screen.getByText('Users')).toBeInTheDocument();
      expect(screen.getByText('500')).toBeInTheDocument();  // âœ“ Verify actual values
    });

    // Assert: API called correctly
    expect(api.fetchMetrics).toHaveBeenCalledTimes(1);
  });

  it('should handle API error gracefully', async () => {
    // Setup: Mock API error
    vi.spyOn(api, 'fetchMetrics').mockRejectedValue(new Error('Network error'));

    // Act: Render component
    render(<DashboardMetrics />);

    // Assert: Error message displayed
    await waitFor(() => {
      expect(screen.getByText(/error loading metrics/i)).toBeInTheDocument();
      expect(screen.getByText(/network error/i)).toBeInTheDocument();  // âœ“ Verify error details
    });
  });
});
```

---

### 3. E2E Tests (10-20% of tests)

**What to Test**:
- Complete user flows (login â†’ dashboard â†’ action â†’ result)
- Critical business paths (checkout, payment, data submission)
- Cross-page navigation and state persistence
- Real browser interactions (forms, clicks, navigation)

**Test Structure** (using Playwright):
```typescript
test.describe('Complete audit workflow', () => {
  test('should create audit, run analysis, and view results', async ({ page }) => {
    // Step 1: Navigate to app
    await page.goto('http://localhost:5173');

    // Step 2: Create new audit
    await page.click('[data-testid="create-audit-btn"]');
    await page.fill('[data-testid="audit-name"]', 'Security Audit 2024');
    await page.selectOption('[data-testid="audit-type"]', 'security');
    await page.click('[data-testid="submit-audit"]');

    // Assert: Audit created
    await expect(page.locator('[data-testid="audit-list"]')).toContainText('Security Audit 2024');

    // Step 3: Run analysis
    await page.click('[data-testid="run-analysis"]');

    // Assert: Analysis running (progress indicator)
    await expect(page.locator('[data-testid="progress-bar"]')).toBeVisible();

    // Step 4: Wait for completion
    await page.waitForSelector('[data-testid="analysis-complete"]', { timeout: 30000 });

    // Assert: Results displayed with actual values
    const resultsText = await page.locator('[data-testid="results-summary"]').textContent();
    expect(resultsText).toMatch(/\d+ issues found/);  // âœ“ Verify result format
    expect(resultsText).toMatch(/\d+ critical/);  // âœ“ Verify specific data

    // Step 5: Verify detailed results
    await page.click('[data-testid="view-details"]');
    const issueCount = await page.locator('[data-testid="issue-item"]').count();
    expect(issueCount).toBeGreaterThan(0);  // âœ“ Verify actual issues rendered

    // Step 6: Verify export functionality
    const downloadPromise = page.waitForEvent('download');
    await page.click('[data-testid="export-results"]');
    const download = await downloadPromise;
    expect(download.suggestedFilename()).toMatch(/security-audit.*\.pdf/);  // âœ“ Verify file
  });
});
```

---

### 4. Coverage Requirements

**Minimum Coverage Targets**:
- **Overall**: 80%+ (lines, statements, branches)
- **Critical paths**: 90%+ (auth, payments, data processing)
- **Utilities/helpers**: 95%+ (pure functions are easy to test)
- **UI components**: 70%+ (focus on behavior, not implementation)

**Coverage Tools**:
```bash
# Frontend (Vitest)
npm run test:coverage

# Backend (pytest)
source venv/bin/activate
pytest --cov=. --cov-report=html

# View coverage report
# Frontend: open coverage/index.html
# Backend: open htmlcov/index.html
```

**Coverage Enforcement** (in PostToolUse hook or CI):
```bash
# Fail if coverage drops below threshold
vitest run --coverage --coverage.branches=80 --coverage.functions=80 --coverage.lines=80
```

---

### 5. Comprehensive Assertions (Not Just "It Works")

**BAD Assertions** (too shallow):
```typescript
// âœ— Doesn't verify actual result
expect(calculateTotal([10, 20])).toBeTruthy();

// âœ— Doesn't verify correct value
expect(formatCurrency(1000)).toBeDefined();

// âœ— Only checks presence, not content
expect(screen.getByTestId('result')).toBeInTheDocument();
```

**GOOD Assertions** (verify actual results):
```typescript
// âœ“ Verifies exact calculation
expect(calculateTotal([10, 20, 30])).toBe(60);

// âœ“ Verifies correct formatting
expect(formatCurrency(1000)).toBe('$1,000.00');

// âœ“ Verifies actual content
expect(screen.getByTestId('result')).toHaveTextContent('Total: $1,000.00');

// âœ“ Verifies structure
const result = parseAPIResponse(mockData);
expect(result).toEqual({
  id: 1,
  name: 'Test',
  metrics: expect.arrayContaining([
    expect.objectContaining({ type: 'revenue' })
  ])
});
```

---

### 6. Testing Workflow Integration

**WORKFLOW 1: Feature Implementation** (Updated):

Phase 4 now includes comprehensive testing:

```bash
4. Testing (BEFORE validation):
   a. Write unit tests for new functions/components
      â†’ Target: 90%+ coverage for new code

   b. Write integration tests for component interactions
      â†’ Target: 80%+ coverage for interactions

   c. Run tests: npm run test
      â†’ Expected: All tests pass

   d. Check coverage: npm run test:coverage
      â†’ Expected: Coverage thresholds met

   e. Manual E2E testing in browser:
      â†’ Test complete user flow
      â†’ Verify edge cases
      â†’ Check error handling
      â†’ Verify result values (not just "it works")

5. Validation & Commit (AFTER testing):
   â†’ /validate-architecture
   â†’ /commit-push-pr
```

**Test-First Development** (Recommended):

```
1. Write failing tests (RED)
   â†’ Define expected behavior

2. Implement feature (GREEN)
   â†’ Make tests pass

3. Refactor (REFACTOR)
   â†’ Improve code quality while tests still pass

4. Verify coverage
   â†’ Ensure targets met
```

---

## Development Environment

### Backend: Python Virtual Environment (venv)

**CRITICAL RULE**: **ALWAYS activate venv before running ANY backend code**.

```bash
# Activate venv (REQUIRED before every backend operation)
source venv/bin/activate

# Now you can run backend commands
python manage.py runserver
pytest
pip install -r requirements.txt

# When done (optional, but good practice)
deactivate
```

**Why venv is MANDATORY**:
- âœ“ Isolates dependencies (prevents global Python pollution)
- âœ“ Ensures correct package versions
- âœ“ Avoids conflicts with system Python
- âœ“ Reproducible environment across machines

**PostToolUse Hook Enforcement**:
The PostToolUse hook detects backend Python files and FAILS if venv is not active:
```bash
# In post-tool-use.sh
if [[ -f "requirements.txt" ]] && [[ -z "$VIRTUAL_ENV" ]]; then
  echo "ERROR: Backend Python detected but venv not activated!"
  echo "Run: source venv/bin/activate"
  exit 1
fi
```

---

### CRITICAL: Prevent Duplicate Server Processes

**NEVER run multiple instances** of the same server (frontend or backend):

```bash
# âœ— BAD - Multiple frontends
Terminal 1: npm run dev  (port 5173)
Terminal 2: npm run dev  (ERROR: port already in use)

# âœ— BAD - Multiple backends
Terminal 1: python manage.py runserver  (port 8000)
Terminal 2: python manage.py runserver  (ERROR: port already in use)
```

**Before starting server, ALWAYS check if already running**:

```bash
# Check frontend (Vite on port 5173)
lsof -ti:5173

# Check backend (Django on port 8000)
lsof -ti:8000

# If running, kill it first
kill -9 $(lsof -ti:5173)  # Frontend
kill -9 $(lsof -ti:8000)  # Backend
```

**Automated Check** (in worktree-setup or before server start):

```bash
# Frontend pre-start check
if lsof -ti:5173 > /dev/null; then
  echo "WARNING: Frontend already running on port 5173"
  echo "Kill it? (y/n)"
  read -r response
  if [[ "$response" == "y" ]]; then
    kill -9 $(lsof -ti:5173)
    echo "Killed existing frontend process"
  else
    echo "Aborting. Cannot start duplicate server."
    exit 1
  fi
fi

# Backend pre-start check (similar)
if lsof -ti:8000 > /dev/null; then
  echo "WARNING: Backend already running on port 8000"
  # ... same logic
fi
```

**Claude Workflow Integration**:

When executing workflows that involve server startup:

```
STEP 1: Check for existing processes
  â†’ lsof -ti:5173 (frontend)
  â†’ lsof -ti:8000 (backend)

STEP 2: If found, ASK USER before killing
  â†’ "Frontend already running. Kill and restart? (y/n)"

STEP 3: Only proceed if user confirms OR no existing process

STEP 4: Start server
  â†’ npm run dev (frontend)
  â†’ source venv/bin/activate && python manage.py runserver (backend)
```

---

### Development Environment Checklist

**Before ANY backend work**:
- [ ] `source venv/bin/activate` (verify with `echo $VIRTUAL_ENV`)
- [ ] Check backend not already running: `lsof -ti:8000`
- [ ] Verify dependencies installed: `pip list`

**Before ANY frontend work**:
- [ ] Check frontend not already running: `lsof -ti:5173`
- [ ] Verify node_modules installed: `ls node_modules`
- [ ] Check correct Node version: `node -v` (should match .nvmrc if exists)

**Before committing**:
- [ ] All tests pass: `npm run test` (frontend), `pytest` (backend)
- [ ] Coverage thresholds met: `npm run test:coverage`, `pytest --cov`
- [ ] No servers running: Kill all dev servers before commit
- [ ] PostToolUse hook passes (includes architecture validation)

---

## Project Context

### Technology Stack

- **Frontend**: React 18.3.1, Vite, TypeScript
- **UI**: Tailwind CSS v4, shadcn/ui (50+ components)
- **Charts**: Recharts
- **Forms**: React Hook Form
- **Icons**: Lucide React, MUI Icons
- **Styling**: OKLch color system, responsive design

### File Organization

```
frontend/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ types/          # TypeScript interfaces
â”‚   â”‚   â”œâ”€â”€ components/     # React components
â”‚   â”‚   â”œâ”€â”€ data/           # Mock data
â”‚   â”‚   â””â”€â”€ App.tsx         # Main app with routing
â”‚   â”œâ”€â”€ styles/             # Global styles
â”‚   â””â”€â”€ main.tsx            # Entry point
â”œâ”€â”€ index.html
â”œâ”€â”€ package.json
â”œâ”€â”€ vite.config.ts
â””â”€â”€ tsconfig.json

root/
â”œâ”€â”€ CLAUDE.md               # This file
â”œâ”€â”€ frontend/               # Frontend application
â”œâ”€â”€ worktrees/              # Feature worktrees (created dynamically)
â””â”€â”€ backend/                # Placeholder for future
```

### Naming Conventions

- **Components**: PascalCase (e.g., `DashboardMetrics.tsx`)
- **Types**: PascalCase interfaces (e.g., `DashboardMetric`, `Task`)
- **Files**: PascalCase for components, camelCase for utilities
- **Branches**: `feature/[description]`, `bugfix/[issue]`, `refactor/[area]`
- **Commits**: `[type]: Description` (feat, fix, refactor, style, docs, test, chore)

### Code Quality Standards

#### Core Principles

1. **Single Responsibility per Function**
   - Each function does ONE thing and does it well
   - If a function name requires "and" or "or", split it
   - Function should be <50 lines, ideally <30 lines
   - Example:
     ```typescript
     // âœ— BAD - Multiple responsibilities
     function processUserAndSaveToDatabase(user: User) {
       const validated = validateUser(user);
       const hashed = hashPassword(validated.password);
       database.save(hashed);
       sendWelcomeEmail(user.email);
     }

     // âœ“ GOOD - Single responsibility
     function processUser(user: User): ProcessedUser {
       const validated = validateUser(user);
       return hashPassword(validated);
     }
     ```

2. **Clarity Over Cleverness**
   - Code should be immediately understandable
   - Prefer explicit over implicit
   - Avoid single-letter variables (except loop indices)
   - Write code for humans, not machines
   - Example:
     ```typescript
     // âœ— BAD - Clever but unclear
     const r = d.filter(x => x.s > 100).map(x => x.n);

     // âœ“ GOOD - Clear and readable
     const highValueCustomers = customers
       .filter(customer => customer.sales > 100)
       .map(customer => customer.name);
     ```

3. **Proactive Class Usage**
   - **Use classes for entities with behavior + state**
   - Classes should be generic and reusable
   - Prefer composition over inheritance
   - Apply OOP patterns: Factory, Strategy, Observer, etc.
   - Example:
     ```typescript
     // âœ— BAD - Procedural with scattered logic
     function calculateAuditScore(data: any) {
       let score = 0;
       // 50 lines of calculation logic...
       return score;
     }

     // âœ“ GOOD - Generic, reusable class
     class AuditScoreCalculator {
       constructor(private strategy: ScoringStrategy) {}

       calculate(audit: Audit): AuditScore {
         return this.strategy.computeScore(audit);
       }

       setStrategy(strategy: ScoringStrategy): void {
         this.strategy = strategy;
       }
     }
     ```

4. **SOLID Principles** (Strictly Enforced)
   - **S**ingle Responsibility: One reason to change
   - **O**pen/Closed: Open for extension, closed for modification
   - **L**iskov Substitution: Subtypes must be substitutable
   - **I**nterface Segregation: Many specific interfaces > one general
   - **D**ependency Inversion: Depend on abstractions, not concretions

5. **Clean Code Principles**
   - **DRY**: Don't Repeat Yourself (extract reusable logic)
   - **KISS**: Keep It Simple, Stupid (simplest solution that works)
   - **YAGNI**: You Aren't Gonna Need It (no speculative features)

6. **No Unnecessary Lines**
   - Every line serves a purpose
   - Remove: unused imports, commented code, empty lines (max 1 consecutive)
   - Remove: debug statements (console.log, debugger)
   - Remove: redundant comments (code should be self-documenting)

#### Size Constraints (Strictly Enforced)

7. **File Size**: **Max 800 lines** per file
   - Split if larger
   - Extract related functionality into modules
   - Use barrel exports (index.ts) for cleaner imports

8. **Component Size**: Max 300 lines per component
   - Extract sub-components if larger
   - Separate business logic into custom hooks
   - Move complex rendering into separate components

9. **Function Size**: Max 50 lines per function
   - Ideally <30 lines
   - Extract helper functions if larger
   - Use early returns to reduce nesting

10. **Function Complexity**: Max 3 levels of nesting
    - Use early returns/guards
    - Extract nested logic into separate functions
    - Prefer flat code over deeply nested

#### Type Safety (Zero Tolerance)

11. **Strict TypeScript**
    - No `any` types (use `unknown` if type truly unknown)
    - No type assertions without runtime validation
    - Enable `strict: true` in tsconfig.json
    - All function parameters and returns typed
    - Example:
      ```typescript
      // âœ— BAD
      function process(data: any): any {
        return data.value;
      }

      // âœ“ GOOD
      function process<T extends { value: string }>(data: T): string {
        return data.value;
      }
      ```

---

## Feedback & Improvement

### Feedback Loop Mechanism

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  1. Execute Workflow                â”‚
â”‚     (plan-implement-verify, etc.)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  2. Capture Feedback                â”‚
â”‚     /feedback-capture [workflow]... â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  3. Update Metrics                  â”‚
â”‚     (workflow-success.json, etc.)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  4. Analyze Patterns                â”‚
â”‚     (weekly or every 10 workflows)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  5. Generate Suggestions            â”‚
â”‚     (improvement-suggestions.md)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  6. Review & Approve                â”‚
â”‚     (user decision)                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  7. Update CLAUDE.md                â”‚
â”‚     (new version with improvements) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  8. Reset Metrics & Repeat          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Monthly Review Checklist

- [ ] Review `~/.claude/metrics/workflow-success.json` - overall success rates
- [ ] Review `~/.claude/metrics/bottlenecks.json` - prioritize resolutions
- [ ] Review `~/.claude/metrics/improvement-suggestions.md` - approve changes
- [ ] Update CLAUDE.md with approved improvements
- [ ] Archive old metrics to `~/.claude/metrics/archive/[month]/`
- [ ] Reset counters for new measurement cycle

---

## Emergency Procedures

### Git Conflicts

```
1. STOP current workflow
2. Bash: git status
3. Identify conflicted files
4. Option A: Resolve in current worktree
   â†’ Edit conflicted files manually
   â†’ git add [resolved-files]
   â†’ git commit
5. Option B: Create recovery worktree
   â†’ /worktree-setup recovery-[issue]
   â†’ Resolve in clean environment
6. Resume workflow
```

### Build Failures

```
1. Read: Error output carefully
2. Bash: npm run build > /tmp/build-errors.log 2>&1
3. Grep: Search codebase for error-related code
4. Spawn Subagent: Fix specific error
5. PostToolUse: Validates fix
6. Retry: npm run build
7. Verify: Build succeeds
```

### Infinite Loops (Ralph-Wiggum)

```
1. Auto-Checkpoint: Ralph-wiggum saves state
2. Review: Last 10 tool calls
3. Identify: Repeating pattern
4. Decision:
   â†’ Simplify: Reduce task scope
   â†’ Delegate: Spawn subagent with different approach
   â†’ Escalate: AskUserQuestion for guidance
5. Update: ~/.claude/metrics/bottlenecks.json
6. Resume: With new strategy
```

### Type Check Failures

```
1. Bash: npx tsc --noEmit > /tmp/tsc-errors.log 2>&1
2. Read: /tmp/tsc-errors.log
3. Identify: Type errors (interface mismatches, missing types)
4. Spawn Subagent: Fix type definitions
5. PostToolUse: Type check validation
6. Verify: All type errors resolved
```

### Architecture Validation Failures

```
1. Read: ~/.claude/metrics/architecture-report.json
2. Categorize issues:
   â†’ Errors: Must fix before commit
   â†’ Warnings: Should fix, tech debt acceptable short-term
3. Fix errors:
   â†’ Remove console.log, debugger, any types
   â†’ Split large files (>800 lines)
   â†’ Extract reusable functions (DRY)
4. Command: /validate-architecture
5. Verify: Errors resolved
```

---

## Best Practices

### Planning

1. âœ“ **Always start in plan mode** unless explicitly told otherwise
2. âœ“ **Research before implementing**: Grep similar patterns, Read reference files
3. âœ“ **Break down tasks granularly**: 5-8 tasks, each <15 min
4. âœ“ **Design before coding**: Think about architecture, OOP patterns, SOLID principles
5. âœ“ **Get user approval**: AskUserQuestion before major implementation

### Implementation

1. âœ“ **Use worktrees**: Isolate feature work with `/worktree-setup`
2. âœ“ **Parallel subagents**: Launch all in SINGLE message for efficiency
3. âœ“ **PostToolUse validation**: Trust the hook, it catches issues early
4. âœ“ **Incremental testing**: Test components as they're built
5. âœ“ **Manual verification**: Always test in browser before committing

### Code Quality

1. âœ“ **Single Responsibility**: Each function does ONE thing, <50 lines (ideally <30)
2. âœ“ **Clarity over Cleverness**: Write explicit, readable code for humans
3. âœ“ **Proactive Class Usage**: Use classes for entities with behavior + state, apply OOP patterns
4. âœ“ **SOLID Principles**: Strictly enforce all five principles
5. âœ“ **Type Safety**: Strict TypeScript, zero `any` types
6. âœ“ **No Unnecessary Lines**: Delete unused code, debug statements, excessive comments
7. âœ“ **File Size Limits**: Max 800 lines per file, 300 per component, 50 per function
8. âœ“ **Code Reuse**: Extract duplicated logic (DRY), create reusable classes

### Git Workflow

1. âœ“ **Atomic commits**: One logical change per commit
2. âœ“ **Descriptive messages**: Explain WHY, not just WHAT
3. âœ“ **Clean history**: Rebase before pushing to avoid merge commits
4. âœ“ **Validate before commit**: `/validate-architecture` passes
5. âœ“ **Link issues**: Reference issue numbers in commits and PRs

### Feedback

1. âœ“ **Always capture**: Run `/feedback-capture` after every workflow
2. âœ“ **Be specific**: Include duration, subagent count, observations
3. âœ“ **Review metrics**: Monthly review of success rates and bottlenecks
4. âœ“ **Act on suggestions**: Implement approved improvements from metrics analysis
5. âœ“ **Iterate**: CLAUDE.md evolves based on real usage

---

## Quick Reference

### Most Used Tool Sequences

**Start Feature**:
```
/worktree-setup [name] â†’ Grep patterns â†’ Read refs â†’ Think design â†’ Spawn subagents â†’ /validate-architecture â†’ /commit-push-pr â†’ /feedback-capture
```

**Fix Bug**:
```
Grep error â†’ Read files â†’ git log â†’ Think fix â†’ /worktree-setup bugfix-[id] â†’ Spawn subagent â†’ /validate-architecture â†’ /commit-push-pr
```

**Refactor**:
```
/validate-architecture â†’ Read report â†’ Think clean architecture â†’ /worktree-setup refactor-[area] â†’ Spawn subagents â†’ /validate-architecture â†’ Compare before/after â†’ /commit-push-pr
```

### Essential File Paths

- **This guide**: `/Users/jaewookim/Desktop/Personal Coding/AI Audit/CLAUDE.md`
- **Hooks**: `~/.claude/hooks/*.sh`, `~/.claude/hooks/*.js`
- **Commands**: `~/.claude/commands/*.md`
- **Metrics**: `~/.claude/metrics/*.json`, `~/.claude/metrics/*.md`
- **Config**: `~/.claude/hooks/hook-config.json`

### Support

**For issues or improvements**:
1. Check Emergency Procedures above
2. Review `~/.claude/metrics/improvement-suggestions.md`
3. Consult global slash commands in `~/.claude/commands/`
4. Update this CLAUDE.md with new learnings

---

**Remember**: This document is LIVING and should evolve based on feedback loops. Trust the metrics, improve continuously, and always start in plan mode. ğŸš€
